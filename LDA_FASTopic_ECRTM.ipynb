{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d09f542-ef70-4bad-9ca6-ba1113f286b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rod/Documents/Taller de Título/topic_models/v3_topic/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "loading train texts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3573.67it/s]\n",
      "parsing texts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 8446.45it/s]\n",
      "/home/rod/Documents/Taller de Título/topic_models/v3_topic/lib64/python3.9/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2025-12-29 10:13:48,566 - TopMost - Real vocab size: 2000\n",
      "2025-12-29 10:13:48,567 - TopMost - Real training size: 1000 \t avg length: 45.851\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import topmost\n",
    "from topmost.data import RawDataset\n",
    "from topmost.preprocess import Preprocess\n",
    "from topmost.trainers import BasicTrainer, FASTopicTrainer\n",
    "from topmost.models import ECRTM\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "docs = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))['data'][:1000]\n",
    "\n",
    "preprocess = Preprocess(vocab_size=2000)\n",
    "dataset_raw = RawDataset(docs, preprocess, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b90dac31-52ca-43d6-89ad-d5dbdfbc2e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. LDA Top Words:\n",
      "Topic 0: [':', 'on', 'with', 'was', 'use', 'this', 'as', 'by', 'from', 'are']\n",
      "Topic 1: ['they', 'with', 'was', 'are', 'not', 'had', 'you', 'would', 'on', 'be']\n",
      "Topic 2: ['|', 'you', 'your', '/', 'if', 'be', 'have', 'can', 'or', '--']\n",
      "Topic 3: ['-', 'you', 'not', 'but', 'we', 'as', 'with', 'are', 'have', 'be']\n",
      "Topic 4: ['have', 'my', 'are', 'or', 'on', 'but', 'this', 'was', 'at', 'with']\n",
      "Topic 5: ['you', 'this', 'not', 'be', 'are', 'have', 'as', '1', 'on', 'was']\n",
      "Topic 6: ['from', 'not', '-', 'but', 'was', '1', 'by', 'spirit', 'has', 'this']\n",
      "Topic 7: ['=', '-', 'on', 'space', '}', '*', 'with', 'shuttle', 'from', 'will']\n",
      "Topic 8: ['with', 'be', 'on', 'have', 'this', 'you', 'can', 'if', 'not', 'but']\n",
      "Topic 9: ['.', 'were', 'they', 'was', 'on', 'by', 'their', 'at', 'we', 'be']\n"
     ]
    }
   ],
   "source": [
    "# 1. LDA con Gensim\n",
    "tokenized_docs = [doc.lower().split() for doc in docs if doc.strip()]\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus_gensim = [dictionary.doc2bow(text) for text in tokenized_docs]\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus_gensim, num_topics=10, id2word=dictionary, passes=10, iterations=100)\n",
    "top_words_lda = lda_model.show_topics(num_topics=10, num_words=10, formatted=False)\n",
    "print(\"\\n1. LDA Top Words:\")\n",
    "for topic_id, words in top_words_lda:\n",
    "    print(f\"Topic {topic_id}: {[word for word, prob in words]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fec1c31-edd4-4247-a880-5b994c5bdc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 10:13:58,677 - FASTopic - use device: cpu\n",
      "2025-12-29 10:13:58,678 - FASTopic - First fit the model.\n",
      "loading train texts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 13357.78it/s]\n",
      "parsing texts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 16036.89it/s]\n",
      "2025-12-29 10:14:06,932 - TopMost - Real vocab size: 2000\n",
      "2025-12-29 10:14:06,932 - TopMost - Real training size: 1000 \t avg length: 45.851\n",
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:08<00:00,  4.00it/s]\n",
      "Training FASTopic:   4%|████▍                                                                                                           | 8/200 [00:00<00:09, 20.86it/s]2025-12-29 10:14:15,510 - FASTopic - Epoch: 010 loss: 348.238\n",
      "Training FASTopic:   8%|█████████▍                                                                                                     | 17/200 [00:00<00:09, 19.66it/s]2025-12-29 10:14:16,035 - FASTopic - Epoch: 020 loss: 340.033\n",
      "Training FASTopic:  14%|████████████████                                                                                               | 29/200 [00:01<00:09, 17.89it/s]2025-12-29 10:14:16,645 - FASTopic - Epoch: 030 loss: 334.107\n",
      "Training FASTopic:  20%|█████████████████████▋                                                                                         | 39/200 [00:02<00:08, 18.31it/s]2025-12-29 10:14:17,205 - FASTopic - Epoch: 040 loss: 329.304\n",
      "Training FASTopic:  24%|███████████████████████████▏                                                                                   | 49/200 [00:02<00:08, 18.57it/s]2025-12-29 10:14:17,743 - FASTopic - Epoch: 050 loss: 326.181\n",
      "Training FASTopic:  30%|████████████████████████████████▋                                                                              | 59/200 [00:03<00:07, 18.62it/s]2025-12-29 10:14:18,282 - FASTopic - Epoch: 060 loss: 324.101\n",
      "Training FASTopic:  34%|██████████████████████████████████████▎                                                                        | 69/200 [00:03<00:06, 18.83it/s]2025-12-29 10:14:18,820 - FASTopic - Epoch: 070 loss: 322.595\n",
      "Training FASTopic:  40%|███████████████████████████████████████████▊                                                                   | 79/200 [00:04<00:06, 17.97it/s]2025-12-29 10:14:19,376 - FASTopic - Epoch: 080 loss: 321.305\n",
      "Training FASTopic:  44%|█████████████████████████████████████████████████▍                                                             | 89/200 [00:04<00:06, 18.48it/s]2025-12-29 10:14:19,919 - FASTopic - Epoch: 090 loss: 320.206\n",
      "Training FASTopic:  50%|██████████████████████████████████████████████████████▉                                                        | 99/200 [00:05<00:05, 18.68it/s]2025-12-29 10:14:20,454 - FASTopic - Epoch: 100 loss: 319.321\n",
      "Training FASTopic:  55%|███████████████████████████████████████████████████████████▉                                                  | 109/200 [00:06<00:05, 16.88it/s]2025-12-29 10:14:21,051 - FASTopic - Epoch: 110 loss: 318.578\n",
      "Training FASTopic:  60%|█████████████████████████████████████████████████████████████████▍                                            | 119/200 [00:06<00:04, 18.36it/s]2025-12-29 10:14:21,591 - FASTopic - Epoch: 120 loss: 317.914\n",
      "Training FASTopic:  64%|██████████████████████████████████████████████████████████████████████▉                                       | 129/200 [00:07<00:03, 18.94it/s]2025-12-29 10:14:22,122 - FASTopic - Epoch: 130 loss: 317.295\n",
      "Training FASTopic:  70%|████████████████████████████████████████████████████████████████████████████▍                                 | 139/200 [00:07<00:03, 17.05it/s]2025-12-29 10:14:22,693 - FASTopic - Epoch: 140 loss: 316.720\n",
      "Training FASTopic:  74%|█████████████████████████████████████████████████████████████████████████████████▉                            | 149/200 [00:08<00:02, 18.63it/s]2025-12-29 10:14:23,224 - FASTopic - Epoch: 150 loss: 316.210\n",
      "Training FASTopic:  80%|███████████████████████████████████████████████████████████████████████████████████████▍                      | 159/200 [00:08<00:02, 18.66it/s]2025-12-29 10:14:23,768 - FASTopic - Epoch: 160 loss: 315.743\n",
      "Training FASTopic:  84%|████████████████████████████████████████████████████████████████████████████████████████████▍                 | 168/200 [00:09<00:01, 19.09it/s]2025-12-29 10:14:24,296 - FASTopic - Epoch: 170 loss: 315.300\n",
      "Training FASTopic:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████▉            | 178/200 [00:09<00:01, 19.83it/s]2025-12-29 10:14:24,797 - FASTopic - Epoch: 180 loss: 314.896\n",
      "Training FASTopic:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████▉      | 189/200 [00:10<00:00, 15.95it/s]2025-12-29 10:14:25,467 - FASTopic - Epoch: 190 loss: 314.518\n",
      "Training FASTopic: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 199/200 [00:10<00:00, 18.09it/s]2025-12-29 10:14:26,012 - FASTopic - Epoch: 200 loss: 314.157\n",
      "Training FASTopic: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:11<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: program health data use users control number university line phone medical information public want mail\n",
      "Topic 1: clinton treatment pain doctor soon tax banks body legal medicine economic diet skepticism surrender doctors\n",
      "Topic 2: max memory scsi dos mac video radius simms ram card mode machine color window serial\n",
      "Topic 3: armenian armenians turkish genocide government killed soviet war left children land russian city jews turks\n",
      "Topic 4: crypto powerful marriage homosexual pure flame contradiction homosexuals topics differ escape african jew strip fundamental\n",
      "Topic 5: jesus god argument believe true truth christian matthew bible example spirit think son conclusion point\n",
      "Topic 6: engine car insurance cars keys speed gun bike self carry water miles turn gas oil\n",
      "Topic 7: sale sell wondering price offer looking portable printer battery normal condition asking disks working box\n",
      "Topic 8: season league players play team runs period los mike win encryption game games pittsburgh teams\n",
      "Topic 9: shuttle orbit launch excellent included space missing files nasa mission sun list earth missions fair\n",
      "\n",
      "2. FASTopic Top Words: ['program health data use users control number university line phone medical information public want mail', 'clinton treatment pain doctor soon tax banks body legal medicine economic diet skepticism surrender doctors', 'max memory scsi dos mac video radius simms ram card mode machine color window serial', 'armenian armenians turkish genocide government killed soviet war left children land russian city jews turks', 'crypto powerful marriage homosexual pure flame contradiction homosexuals topics differ escape african jew strip fundamental', 'jesus god argument believe true truth christian matthew bible example spirit think son conclusion point', 'engine car insurance cars keys speed gun bike self carry water miles turn gas oil', 'sale sell wondering price offer looking portable printer battery normal condition asking disks working box', 'season league players play team runs period los mike win encryption game games pittsburgh teams', 'shuttle orbit launch excellent included space missing files nasa mission sun list earth missions fair']\n"
     ]
    }
   ],
   "source": [
    "# 2. FASTopic\n",
    "trainer_fast = FASTopicTrainer(dataset_raw, num_topics=10, verbose=True)\n",
    "top_words_fast, _ = trainer_fast.train()\n",
    "print(\"\\n2. FASTopic Top Words:\", top_words_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a931ecc-ae9a-4a0c-adda-33ef42662814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:14<00:00, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. ECRTM Top Words: ['jesus people matthew radius god christians bible lord double faith passage king christian sin jewish', 'period play team games game pittsburgh jose players chicago los teams power season detroit san', 'health users volume reported persons united medical use report april washington page culture states age', 'father son spirit good excellent holy gun church missing cover council state poster included fair', 'armenian armenians turkish genocide russian soviet people turks muslim armenia killed population women argic government', 'insurance car chastity turbo gordon shameful banks thanks skepticism intellect rate surrender driving sale year', 'nasa shuttle mission space missions images orbit operations earth launch applications data science files military', 'max argument truth example bible conclusion true false god valid absolute belief occurs christians beliefs', 'windows scsi dos bios video memory controller guide disk ram motif ide motherboard microsoft mac', 'thanks video mac battery color ram card clock simms plug cable memory thanx dos keyboard']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. ECRTM\n",
    "model_ecrtm = ECRTM(vocab_size=preprocess.vocab_size, num_topics=10)\n",
    "trainer_ecrtm = BasicTrainer(model_ecrtm, dataset_raw)\n",
    "top_words_ecrtm, _ = trainer_ecrtm.train()\n",
    "print(\"\\n3. ECRTM Top Words:\", top_words_ecrtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb9fa2-73fc-4547-b646-e829792334b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TopicModels)",
   "language": "python",
   "name": "topic-models-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
