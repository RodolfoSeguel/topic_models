{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2ad0ac-b938-4d75-95cd-73a75b58e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "\n",
    "dataset20NG = Dataset()\n",
    "dataset20NG.fetch_dataset(\"20NewsGroup\")\n",
    "\n",
    "# Para una pequeña prueba, tomamos solo los primeros 100 documentos\n",
    "dataset20NG._Dataset__corpus = dataset20NG._Dataset__corpus[:100]\n",
    "if hasattr(dataset20NG, '_Dataset__labels') and dataset20NG._Dataset__labels:\n",
    "    dataset20NG._Dataset__labels = dataset20NG._Dataset__labels[:100]\n",
    "if hasattr(dataset20NG, '_Dataset__partitioned_corpus') and dataset20NG._Dataset__partitioned_corpus:\n",
    "    dataset20NG._Dataset__partitioned_corpus = [part[:100] if i == 0 else part for i, part in enumerate(dataset20NG._Dataset__partitioned_corpus)]\n",
    "\n",
    "# Reconstruimos el vocabulario basado en el subconjunto para que sea consistente\n",
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "for doc in dataset20NG._Dataset__corpus:\n",
    "    vocab.update(doc)\n",
    "\n",
    "dataset20NG._Dataset__vocabulary = sorted(vocab.keys())\n",
    "\n",
    "dataset20NG._Dataset__metadata\n",
    "\n",
    "# Use pickle.dump to serialize the object and write it to the file\n",
    "with open('Preprocessed_Data/octis_dataset_20ng_octis_original_small.pickle', 'wb') as f:\n",
    "    pickle.dump(dataset20NG, f)\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Para una pequeña prueba, tomamos solo los primeros 100 documentos del conjunto completo\n",
    "twng_sklearn = fetch_20newsgroups(subset=\"all\")[\"data\"][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f71a83-8925-4cc0-9a37-893ab2831607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "4938\n"
     ]
    }
   ],
   "source": [
    "with open(\"Raw_Data/twng_data.pickle\", \"wb\") as file:\n",
    "    pickle.dump(twng_sklearn, file)\n",
    "    \n",
    "twng_sklaern2 = [doc.replace('\\n', \" \").replace('\\r', '') for doc in twng_sklearn]\n",
    "twng_sklaern2 = [doc.replace('\\t', \" \") for doc in twng_sklaern2]\n",
    "\n",
    "with open(\"Raw_Data/twng_data.pickle\", \"wb\") as file:\n",
    "    pickle.dump(twng_sklaern2, file)\n",
    "with open('Auxillary_Data/twng_textData.txt', 'w') as handle:\n",
    "    for line in twng_sklaern2:\n",
    "        handle.write(\"%s\\n\" % line\n",
    "    )\n",
    "\n",
    "preprocessor = Preprocessing(\n",
    "    min_df = 0.005,\n",
    "    min_words_docs = 5\n",
    ")\n",
    "        \n",
    "dataset_20ng = preprocessor.preprocess_dataset('Auxillary_Data/twng_textData.txt')\n",
    "dataset_20ng._Dataset__vocabulary = sorted(dataset_20ng._Dataset__vocabulary )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95739919-e664-4df2-9fad-676c90f7b1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus_raw = twng_sklaern2\n",
    "corpus_octis = dataset_20ng.get_corpus()\n",
    "re_idx = dataset_20ng._Dataset__original_indexes\n",
    "\n",
    "print(len(corpus_raw))\n",
    "\n",
    "docs = [line.strip() for line in open('Auxillary_Data/twng_textData.txt', 'r').readlines()]\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "# Write Octis dataset to pickle\n",
    "with open(\"Preprocessed_Data/octis_dataset_20ng.pickle\", \"wb\") as f:\n",
    "    pickle.dump(dataset_20ng, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e71f89-47b1-4563-b11c-6c0441b01ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cpu\n",
      "tokenizing...\n",
      "tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25361/25361 [00:01<00:00, 14119.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 5777.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 88824.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute embeddings of entire corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:46<00:00,  2.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 636.34it/s]\n",
      "/home/rod/Documents/Taller de Título/topic_models/TNTM/Code/Preprocessing/Preprocessor.py:252: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cur.sort_index(inplace =True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute mean embeddings of each individual word...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4938/4938 [00:00<00:00, 7192.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25361/25361 [00:01<00:00, 17320.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute bow representation of each document...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 5608.11it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Taller de Título/topic_models/v3_topic/lib64/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'embedding'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m vocab \u001b[38;5;241m=\u001b[39m data_raw\u001b[38;5;241m.\u001b[39mget_vocabulary()\n\u001b[1;32m     42\u001b[0m word_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(word_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 43\u001b[0m document_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/Documents/Taller de Título/topic_models/v3_topic/lib64/python3.9/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/Taller de Título/topic_models/v3_topic/lib64/python3.9/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'embedding'"
     ]
    }
   ],
   "source": [
    "from Preprocessor import Preprocessor\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(41)\n",
    "device = 'cpu'\n",
    "print(f'current device: {device}')\n",
    "\n",
    "data_raw = pickle.load(open(\"Preprocessed_Data/octis_dataset_20ng.pickle\", \"rb\"))\n",
    "from transformers import BertTokenizer\n",
    "#bert-base-uncased model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "#tokenizer for every single word\n",
    "tokenizer_word = lambda string: tokenizer.encode_plus(\n",
    "text = string,\n",
    "add_special_tokens = False\n",
    ")['input_ids']\n",
    "bert1 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert1.to(device)\n",
    "bert1.eval()\n",
    "bert1.requires_grad_(False)\n",
    "bert_encoder = Preprocessor.encode_bert_wrap(bert1)\n",
    "corpus_raw_str_lis = [\" \".join(doc) for doc in data_raw.get_corpus()]\n",
    "prep = Preprocessor(corpus_raw_str_lis)\n",
    "res = prep.compute_embeddings(\n",
    "    tokenizer_word = tokenizer_word,\n",
    "    transformer_chunk_len = 512 - 2,\n",
    "    encoder = bert_encoder,\n",
    "    device = device,\n",
    ")\n",
    "\n",
    "df, word_df, cleaned_bow_worddata = res\n",
    "word_df.sort_index(inplace = True)\n",
    "\n",
    "with open(\"../../Data/DataResults_BERT/cleaned_bow_worddata_20ng_BERT.pickle\", \"wb\") as file:\n",
    "    pickle.dump(cleaned_bow_worddata, file)\n",
    "with open(\"../../Data/DataResults_BERT/cleaned_embedding_df_20ng_BERT.pickle\", \"wb\") as file:\n",
    "    pickle.dump(word_df, file)\n",
    "\n",
    "corpus = data_raw.get_corpus()\n",
    "vocab = data_raw.get_vocabulary()\n",
    "word_embeddings = torch.stack(word_df['embedding'].tolist())\n",
    "document_embeddings = torch.stack(df['embedding'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd2079-fd17-410a-a66e-86122bffd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TNTM_SentenceTransformer\n",
    "tntm = TNTM_SentenceTransformer.TNTM_SentenceTransformer(\n",
    "    n_topics = 20,\n",
    "    save_path = f\"example/{20}_topics\",\n",
    "    enc_lr = 1e-3,\n",
    "    dec_lr = 1e-3\n",
    ")\n",
    "result = tntm.fit(\n",
    "    corpus = corpus,\n",
    "    vocab = vocab,\n",
    "    word_embeddings = word_embeddings,\n",
    "    document_embeddings = document_embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde5b47-feda-4a8a-a213-b9cbafbe6d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TopicModels)",
   "language": "python",
   "name": "topic-models-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
