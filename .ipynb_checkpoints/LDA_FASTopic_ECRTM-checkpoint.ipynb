{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d09f542-ef70-4bad-9ca6-ba1113f286b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading train texts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3862.81it/s]\n",
      "parsing texts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 9781.58it/s]\n",
      "2025-12-29 09:33:55,656 - TopMost - Real vocab size: 2000\n",
      "2025-12-29 09:33:55,656 - TopMost - Real training size: 1000 \t avg length: 45.852\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import topmost\n",
    "from topmost.data import RawDataset\n",
    "from topmost.preprocess import Preprocess\n",
    "from topmost.trainers import BasicTrainer, FASTopicTrainer\n",
    "from topmost.models import ECRTM\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "docs = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))['data'][:1000]\n",
    "\n",
    "preprocess = Preprocess(vocab_size=2000)\n",
    "dataset_raw = RawDataset(docs, preprocess, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b90dac31-52ca-43d6-89ad-d5dbdfbc2e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. LDA Top Words:\n",
      "Topic 0: [':', 'on', 'with', 'was', 'use', 'this', 'as', 'by', 'from', 'are']\n",
      "Topic 1: ['they', 'with', 'was', 'are', 'not', 'had', 'you', 'would', 'on', 'be']\n",
      "Topic 2: ['|', 'you', 'your', '/', 'if', 'be', 'have', 'can', 'or', '--']\n",
      "Topic 3: ['-', 'you', 'not', 'but', 'we', 'as', 'with', 'are', 'have', 'be']\n",
      "Topic 4: ['have', 'my', 'are', 'or', 'on', 'but', 'this', 'was', 'at', 'with']\n",
      "Topic 5: ['you', 'this', 'not', 'be', 'are', 'have', 'as', '1', 'on', 'was']\n",
      "Topic 6: ['from', 'not', '-', 'but', 'was', '1', 'by', 'spirit', 'has', 'this']\n",
      "Topic 7: ['=', '-', 'on', 'space', '}', '*', 'with', 'shuttle', 'from', 'will']\n",
      "Topic 8: ['with', 'be', 'on', 'have', 'this', 'you', 'can', 'if', 'not', 'but']\n",
      "Topic 9: ['.', 'were', 'they', 'was', 'on', 'by', 'their', 'at', 'we', 'be']\n"
     ]
    }
   ],
   "source": [
    "# 1. LDA con Gensim\n",
    "tokenized_docs = [doc.lower().split() for doc in docs if doc.strip()]\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus_gensim = [dictionary.doc2bow(text) for text in tokenized_docs]\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus_gensim, num_topics=10, id2word=dictionary, passes=10, iterations=100)\n",
    "top_words_lda = lda_model.show_topics(num_topics=10, num_words=10, formatted=False)\n",
    "print(\"\\n1. LDA Top Words:\")\n",
    "for topic_id, words in top_words_lda:\n",
    "    print(f\"Topic {topic_id}: {[word for word, prob in words]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fec1c31-edd4-4247-a880-5b994c5bdc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 09:34:05,304 - FASTopic - use device: cpu\n",
      "2025-12-29 09:34:05,306 - FASTopic - First fit the model.\n",
      "loading train texts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 10577.76it/s]\n",
      "parsing texts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 16118.67it/s]\n",
      "2025-12-29 09:34:08,872 - TopMost - Real vocab size: 2000\n",
      "2025-12-29 09:34:08,873 - TopMost - Real training size: 1000 \t avg length: 45.852\n",
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:06<00:00,  5.11it/s]\n",
      "Training FASTopic:   2%|██▊                                                                                                             | 5/200 [00:00<00:04, 41.94it/s]2025-12-29 09:34:15,472 - FASTopic - Epoch: 010 loss: 348.586\n",
      "Training FASTopic:   9%|█████████▉                                                                                                     | 18/200 [00:00<00:06, 29.31it/s]2025-12-29 09:34:15,869 - FASTopic - Epoch: 020 loss: 339.367\n",
      "Training FASTopic:  14%|██████████████▉                                                                                                | 27/200 [00:01<00:07, 21.89it/s]2025-12-29 09:34:16,385 - FASTopic - Epoch: 030 loss: 332.078\n",
      "Training FASTopic:  20%|█████████████████████▋                                                                                         | 39/200 [00:01<00:08, 18.28it/s]2025-12-29 09:34:17,007 - FASTopic - Epoch: 040 loss: 327.895\n",
      "Training FASTopic:  24%|███████████████████████████▏                                                                                   | 49/200 [00:02<00:09, 16.16it/s]2025-12-29 09:34:17,655 - FASTopic - Epoch: 050 loss: 325.262\n",
      "Training FASTopic:  30%|████████████████████████████████▋                                                                              | 59/200 [00:02<00:08, 17.50it/s]2025-12-29 09:34:18,220 - FASTopic - Epoch: 060 loss: 323.266\n",
      "Training FASTopic:  34%|██████████████████████████████████████▎                                                                        | 69/200 [00:03<00:07, 17.79it/s]2025-12-29 09:34:18,783 - FASTopic - Epoch: 070 loss: 321.746\n",
      "Training FASTopic:  40%|███████████████████████████████████████████▊                                                                   | 79/200 [00:04<00:06, 18.20it/s]2025-12-29 09:34:19,336 - FASTopic - Epoch: 080 loss: 320.523\n",
      "Training FASTopic:  44%|█████████████████████████████████████████████████▍                                                             | 89/200 [00:04<00:06, 18.50it/s]2025-12-29 09:34:19,894 - FASTopic - Epoch: 090 loss: 319.480\n",
      "Training FASTopic:  50%|██████████████████████████████████████████████████████▉                                                        | 99/200 [00:05<00:06, 16.39it/s]2025-12-29 09:34:20,529 - FASTopic - Epoch: 100 loss: 318.601\n",
      "Training FASTopic:  55%|███████████████████████████████████████████████████████████▉                                                  | 109/200 [00:05<00:05, 17.99it/s]2025-12-29 09:34:21,081 - FASTopic - Epoch: 110 loss: 317.836\n",
      "Training FASTopic:  60%|█████████████████████████████████████████████████████████████████▍                                            | 119/200 [00:06<00:04, 17.80it/s]2025-12-29 09:34:21,658 - FASTopic - Epoch: 120 loss: 317.155\n",
      "Training FASTopic:  64%|██████████████████████████████████████████████████████████████████████▉                                       | 129/200 [00:06<00:03, 18.30it/s]2025-12-29 09:34:22,214 - FASTopic - Epoch: 130 loss: 316.561\n",
      "Training FASTopic:  70%|████████████████████████████████████████████████████████████████████████████▍                                 | 139/200 [00:07<00:03, 17.98it/s]2025-12-29 09:34:22,771 - FASTopic - Epoch: 140 loss: 316.008\n",
      "Training FASTopic:  74%|█████████████████████████████████████████████████████████████████████████████████▉                            | 149/200 [00:08<00:02, 18.25it/s]2025-12-29 09:34:23,321 - FASTopic - Epoch: 150 loss: 315.458\n",
      "Training FASTopic:  80%|███████████████████████████████████████████████████████████████████████████████████████▍                      | 159/200 [00:08<00:02, 18.10it/s]2025-12-29 09:34:23,882 - FASTopic - Epoch: 160 loss: 314.947\n",
      "Training FASTopic:  84%|████████████████████████████████████████████████████████████████████████████████████████████▉                 | 169/200 [00:09<00:01, 17.77it/s]2025-12-29 09:34:24,455 - FASTopic - Epoch: 170 loss: 314.492\n",
      "Training FASTopic:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████▍           | 179/200 [00:09<00:01, 17.80it/s]2025-12-29 09:34:25,028 - FASTopic - Epoch: 180 loss: 314.077\n",
      "Training FASTopic:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████▉      | 189/200 [00:10<00:00, 12.97it/s]2025-12-29 09:34:25,732 - FASTopic - Epoch: 190 loss: 313.698\n",
      "Training FASTopic: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 199/200 [00:11<00:00, 16.68it/s]2025-12-29 09:34:26,320 - FASTopic - Epoch: 200 loss: 313.346\n",
      "Training FASTopic: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:11<00:00, 17.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: jesus god argument truth matthew christian bible son spirit conclusion believe true example church faith\n",
      "Topic 1: simms radius window memory screen color motif bios display clock machine problems running dos write\n",
      "Topic 2: scsi space shuttle mac video nasa launch orbit mission cable sale offer price asking board\n",
      "Topic 3: max windows program data health file information users address files systems medical mail images access\n",
      "Topic 4: good got time dont didnt look sure going like think right thats thing better gun\n",
      "Topic 5: doctor soon banks gordon skepticism doctors diet chastity intellect shameful surrender effective flame trip thank\n",
      "Topic 6: season players league play mike team runs teams los pittsburgh period clinton toronto win san\n",
      "Topic 7: armenian armenians turkish genocide killed soviet russian war turks muslim population army government jews today\n",
      "Topic 8: insurance keys cars engine air speed bike car rate miles oil water model rates driving\n",
      "Topic 9: values effect sex theory humans visible legal certainly treatment talking objective schools increases billion funny\n",
      "\n",
      "2. FASTopic Top Words: ['jesus god argument truth matthew christian bible son spirit conclusion believe true example church faith', 'simms radius window memory screen color motif bios display clock machine problems running dos write', 'scsi space shuttle mac video nasa launch orbit mission cable sale offer price asking board', 'max windows program data health file information users address files systems medical mail images access', 'good got time dont didnt look sure going like think right thats thing better gun', 'doctor soon banks gordon skepticism doctors diet chastity intellect shameful surrender effective flame trip thank', 'season players league play mike team runs teams los pittsburgh period clinton toronto win san', 'armenian armenians turkish genocide killed soviet russian war turks muslim population army government jews today', 'insurance keys cars engine air speed bike car rate miles oil water model rates driving', 'values effect sex theory humans visible legal certainly treatment talking objective schools increases billion funny']\n"
     ]
    }
   ],
   "source": [
    "# 2. FASTopic\n",
    "trainer_fast = FASTopicTrainer(dataset_raw, num_topics=10, verbose=True)\n",
    "top_words_fast, _ = trainer_fast.train()\n",
    "print(\"\\n2. FASTopic Top Words:\", top_words_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a931ecc-ae9a-4a0c-adda-33ef42662814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:15<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. ECRTM Top Words: ['jesus matthew people armenians father lord city armenian gun passage burned said israel soldiers king', 'scsi files radius dos images bios ram windows disk floppy mac motif directory video gif', 'max health mission shuttle nasa period windows los missions space orbit chicago launch pittsburgh play', 'thanks sale portable looking printer battery video card display brand price adapter cpu recommend server', 'argument father conclusion true son example spirit false bible truth valid god holy faith church', 'thanks mon printer sale looking portable video cpu display devils pittsburgh shameful chastity battery intellect', 'excellent missing good cover poster fair included gods indicates pressure condition index update flow game', 'thanks sale video printer portable looking card display battery color cpu ram brand ide appreciated', 'armenian turkish armenians people genocide turks health soviet russian killed muslim armenia army women argic', 'jesus truth god absolute people bible human sin eternal christ gods revelation reality christians peter']\n"
     ]
    }
   ],
   "source": [
    "# 3. ECRTM\n",
    "model_ecrtm = ECRTM(vocab_size=preprocess.vocab_size, num_topics=10)\n",
    "trainer_ecrtm = BasicTrainer(model_ecrtm, dataset_raw)\n",
    "top_words_ecrtm, _ = trainer_ecrtm.train()\n",
    "print(\"\\n3. ECRTM Top Words:\", top_words_ecrtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb9fa2-73fc-4547-b646-e829792334b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TopicModels)",
   "language": "python",
   "name": "topic-models-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
